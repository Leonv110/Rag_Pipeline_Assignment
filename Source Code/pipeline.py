import os
import hashlib
from dotenv import load_dotenv
import ollama
from typing import List
import re

# Helper modules
from data_connectors import (
    get_pg_user_data, get_pg_order_data, get_mongo_project_data,
    get_api_country_data, get_redis_cache, set_redis_cache
)
from router import route_query

# Load environment variables
load_dotenv()

# -----------------------------------------------------------------------
# LLM CONFIGURATION (Ollama)
# -----------------------------------------------------------------------
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_MODEL_NAME = os.getenv("OLLAMA_MODEL_NAME", "llama3")

# -----------------------------------------------------------------------
# Retrieval Function Mapping
# -----------------------------------------------------------------------
RETRIEVAL_FUNCTION_MAP = {
    "PostgreSQL_User": get_pg_user_data,
    "PostgreSQL_Order": get_pg_order_data,
    "MongoDB_Project": get_mongo_project_data,
    "REST_API_Country": get_api_country_data,
}

# -----------------------------------------------------------------------
# 1. LLM Generation (Ollama Implementation)
# -----------------------------------------------------------------------
def generate_llm_response(context: str, query: str) -> str:
    """
    Generates the final response using the Ollama model.

    Parameters
    ----------
    context : str
        The combined retrieved context from all data sources.
    query : str
        The user's original question.

    Returns
    -------
    str
        The text output generated by the LLM. Returns an error message if
        the model invocation fails.
    """
    try:
        system_prompt = (
            "You are a retrieval-augmented generation (RAG) assistant. Your task is to answer "
            "the user's question strictly using the provided CONTEXT. Do not introduce external "
            "information. If the context does not contain the required information, state that "
            "the data is unavailable in the connected data sources."
        )

        user_prompt = (
            f"CONTEXT:\n---\n{context}\n---\n"
            f"QUESTION: {query}"
        )

        client = ollama.Client(host=OLLAMA_API_URL)

        response = client.chat(
            model=OLLAMA_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )

        return response["message"]["content"]

    except Exception as e:
        return (
            f"LLM_ERROR: Unable to generate output from Ollama. "
            f"Ensure that Ollama is running and the model '{OLLAMA_MODEL_NAME}' "
            f"is available. Error: {e}"
        )


# -----------------------------------------------------------------------
# 2. Main RAG Pipeline
# -----------------------------------------------------------------------
def run_rag_pipeline(query: str) -> str:
    """
    Executes the full RAG pipeline including routing, retrieval, context
    construction, caching, and LLM response generation.

    Parameters
    ----------
    query : str
        The user's input question.

    Returns
    -------
    str
        The final answer generated by the RAG system.
    """
    # Create a unique hash for caching
    query_hash = hashlib.sha256(query.encode()).hexdigest()

    # Step 1: Check Redis Cache
    cached_response = get_redis_cache(query_hash)
    if cached_response:
        print("Cache HIT. Returning cached response.")
        return cached_response

    # Step 2: Determine required sources
    source_keys = route_query(query)

    # Step 3: Retrieve and build context
    combined_context: List[str] = []

    for key in source_keys:
        retrieval_function = RETRIEVAL_FUNCTION_MAP.get(key)
        if not retrieval_function:
            continue

        print(f"Retrieving data from: {key}")

        # Parameter Extraction (source-specific)
        query_param = query

        if key == "PostgreSQL_User":
            # Extract username pattern 'user_10'
            match = re.search(r"'(user_\d+)'", query)
            query_param = match.group(1) if match else "user_10"

        elif key == "MongoDB_Project":
            if "AI RAG Pipeline" in query:
                query_param = "AI RAG Pipeline"
            elif "pipeline" in query:
                query_param = "pipeline"

        elif key == "REST_API_Country":
            query_param = "Germany"

        # Retrieve data
        retrieved_data = retrieval_function(query_param)

        # Skip errors from connectors
        if "_ERROR:" in retrieved_data:
            print(f"Retrieval Error for {key}: {retrieved_data}")
            continue

        combined_context.append(f"<{key}>: {retrieved_data}")

    # Combine all context
    context_str = "\n".join(combined_context)

    # Step 4: Trim context if too large
    MAX_CONTEXT_CHARS = 10000
    if len(context_str) > MAX_CONTEXT_CHARS:
        print("Context truncated due to size limit.")
        context_str = context_str[:MAX_CONTEXT_CHARS] + "\n[CONTEXT TRUNCATED]"

    if not context_str.strip():
        return "No relevant information could be retrieved from any data source."

    print(f"Context Prepared (Length: {len(context_str)} characters).")

    # Step 5: Generate final answer via LLM
    final_answer = generate_llm_response(context_str, query)

    # Step 6: Store in Redis Cache
    set_redis_cache(query_hash, final_answer)

    return final_answer


# -----------------------------------------------------------------------
# MAIN EXECUTION (Testing Block)
# -----------------------------------------------------------------------
if __name__ == "__main__":

    query_1 = "What is the role and department of the user named 'user_1'?"
    query_2 = ("Show me project pipeline details," "including the department of the employee who might be on the team.")
    query_3 = "What is the population of Germany?"

    print("\n" + "=" * 80)
    print("TEST 1: Single Source Retrieval (PostgreSQL)")
    response_1 = run_rag_pipeline(query_1)
    print(response_1)

    print("\n" + "=" * 80)
    print("TEST 2: Multi-Source Retrieval (PostgreSQL + MongoDB)")
    response_2 = run_rag_pipeline(query_2)
    print(response_2)

    print("\n" + "=" * 80)
    print("TEST 3: REST API Data Retrieval")
    response_3 = run_rag_pipeline(query_3)
    print(response_3)

    print("\n" + "=" * 80)
    print("TEST 4: Cache Verification (Query 3 again)")
    response_4 = run_rag_pipeline(query_3)
    print(response_4)
